{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vcu-N4onQ47"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm445t0Oor1z"
      },
      "source": [
        "# Import Training Data and do some exploratory analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og3jJ9YbkUx-",
        "outputId": "91c5d3a8-b11d-4865-8525-af4b6d17f0f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HJBqut9Ty5hUKnohAlWY-i-p4vz3BLSS\n",
            "To: /content/train.csv\n",
            "100% 33.1M/33.1M [00:00<00:00, 56.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hAZW6ZSboAMq073E0y6Uz1f_T2I12BGR\n",
            "To: /content/X.npy\n",
            "100% 4.84M/4.84M [00:00<00:00, 21.1MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=11aderwsGMZvaYf_mgjuvbewTZRI2EWeY\n",
            "To: /content/y.npy\n",
            "100% 2.03k/2.03k [00:00<00:00, 8.19MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1QOv6Kby9CqHIt5CCshNBJutjExsFKu2L\n",
            "To: /content/X_interpo&fill_nan_median.npy\n",
            "100% 5.15M/5.15M [00:00<00:00, 84.5MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1DJ0RdlLGZR4OkSTNU1yL6zp5U9C-s_Tl\n",
            "To: /content/X_test_interpo&fill_nan_median.npy\n",
            "100% 2.70M/2.70M [00:00<00:00, 127MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F6H2xVEBobtUoqEKk5g2ST6QVN6oe-1p\n",
            "To: /content/asset_id_corresponding_to_X_test.csv\n",
            "100% 4.53k/4.53k [00:00<00:00, 18.6MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1tN5j9IvaUI_b1JCGrgY_Za9bLXHcm7oj\n",
            "To: /content/X_test_fully_scaled_preprocessed_with_time_index.npy\n",
            "100% 9.16M/9.16M [00:00<00:00, 19.3MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:132: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1PPUtdyH7Xl9HxCwkuaK7znB1AKpZuEnC\n",
            "To: /content/X_train_fully_scaled_preprocessed_with_time_index.npy\n",
            "100% 17.4M/17.4M [00:00<00:00, 43.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "#https://drive.google.com/file/d/1HJBqut9Ty5hUKnohAlWY-i-p4vz3BLSS/view?usp=sharing\n",
        "!gdown --id 1HJBqut9Ty5hUKnohAlWY-i-p4vz3BLSS # train.csv\n",
        "\n",
        "#https://drive.google.com/file/d/1hAZW6ZSboAMq073E0y6Uz1f_T2I12BGR/view?usp=sharing\n",
        "!gdown --id 1hAZW6ZSboAMq073E0y6Uz1f_T2I12BGR # X.npy\n",
        "\n",
        "#https://drive.google.com/file/d/11aderwsGMZvaYf_mgjuvbewTZRI2EWeY/view?usp=sharing\n",
        "!gdown --id 11aderwsGMZvaYf_mgjuvbewTZRI2EWeY # y.npy\n",
        "\n",
        "#https://drive.google.com/file/d/1QOv6Kby9CqHIt5CCshNBJutjExsFKu2L/view?usp=sharing\n",
        "!gdown --id 1QOv6Kby9CqHIt5CCshNBJutjExsFKu2L # X_interpro&fill_nan_median\n",
        "\n",
        "#https://drive.google.com/file/d/1DJ0RdlLGZR4OkSTNU1yL6zp5U9C-s_Tl/view?usp=sharing\n",
        "\n",
        "!gdown --id 1DJ0RdlLGZR4OkSTNU1yL6zp5U9C-s_Tl # X_test_interpro&fill_nan_median\n",
        "\n",
        "\n",
        "\n",
        "#https://drive.google.com/file/d/1F6H2xVEBobtUoqEKk5g2ST6QVN6oe-1p/view?usp=sharing\n",
        "!gdown --id 1F6H2xVEBobtUoqEKk5g2ST6QVN6oe-1p #asset_id_corresponding_to_X_test\n",
        "\n",
        "#https://drive.google.com/file/d/1tN5j9IvaUI_b1JCGrgY_Za9bLXHcm7oj/view?usp=sharing\n",
        "!gdown --id 1tN5j9IvaUI_b1JCGrgY_Za9bLXHcm7oj #X_test_fully_scaled_preprocessed_with_time_index\n",
        "\n",
        "#https://drive.google.com/file/d/1PPUtdyH7Xl9HxCwkuaK7znB1AKpZuEnC/view?usp=sharing\n",
        "!gdown --id 1PPUtdyH7Xl9HxCwkuaK7znB1AKpZuEnC #X_train_fully_scaled_preprocessed_with_time_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66F7JgADFY41"
      },
      "outputs": [],
      "source": [
        "X = np.load('X_train_fully_scaled_preprocessed_with_time_index.npy')\n",
        "y = np.load('y.npy')\n",
        "\n",
        "X = np.nan_to_num(X, nan=0.0)\n",
        "\n",
        "\n",
        "assert not np.isnan(X).any(), \"X contains NaN values\"\n",
        "assert not np.isnan(y).any(), \"y contains NaN values\"\n",
        "assert not np.isinf(X).any(), \"X contains Inf values\"\n",
        "assert not np.isinf(y).any(), \"y contains Inf values\"\n",
        "\n",
        "X_test = np.load('X_test_fully_scaled_preprocessed_with_time_index.npy')\n",
        "\n",
        "df_test = pd.read_csv('asset_id_corresponding_to_X_test.csv')\n",
        "\n",
        "X_test = np.nan_to_num(X_test, nan=0.0)\n",
        "\n",
        "\n",
        "assert not np.isnan(X_test).any(), \"X contains NaN values\"\n",
        "assert not np.isinf(X_test).any(), \"y contains Inf values\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXMr8efyA-8s",
        "outputId": "d09be2ae-6c8f-4863-d30e-c9df02425fb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238, 229, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "715Hsb0UBT0L",
        "outputId": "c6aae36c-f22a-439a-b144-2e3e30b2db11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(238, 229, 40)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-GEQ6RrWNkY"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "n_embd = X.shape[2] * X.shape[1]\n",
        "T = X.shape[1]\n",
        "C = X.shape[2]\n",
        "max_seq_length = n_embd\n",
        "n_layer = 8 #4\n",
        "n_head = 4  #4\n",
        "n_hidden = 1000\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gbgNLe5GG7W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, X, y=None, augment=True):\n",
        "        self.X = torch.tensor(X, dtype=torch.float64)\n",
        "        self.y = torch.tensor(y, dtype=torch.float64) if y is not None else None\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.X[idx]\n",
        "        y = self.y[idx] if self.y is not None else None\n",
        "\n",
        "        if self.augment and y is not None:\n",
        "            x = self.augment_data(x)\n",
        "\n",
        "        if y is not None:\n",
        "            return x, y\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "    def augment_data(self, x):\n",
        "        shift = np.random.randint(-5, 6)\n",
        "        x = torch.roll(x, shifts=shift, dims=0)\n",
        "\n",
        "        for i in range(x.shape[1]):\n",
        "            shift = np.random.randint(-5, 6)\n",
        "            x[:, i] = torch.roll(x[:, i], shifts=shift)\n",
        "\n",
        "        noise = torch.normal(0, 0.01, x.shape)\n",
        "        x += noise\n",
        "\n",
        "        multiplier = torch.tensor(np.random.uniform(0.95, 1.05, x.shape[1]), dtype=torch.float64)\n",
        "        x *= multiplier\n",
        "\n",
        "        return x\n",
        "\n",
        "# Create dataset\n",
        "dataset = TimeSeriesDataset(X, y)\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CukV7tYOQOaZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class HeadEncoder(nn.Module):\n",
        "    def __init__(self, n_embd, head_size, T, C, window_size=300):\n",
        "        super().__init__()\n",
        "        self.n_embd = n_embd\n",
        "        self.head_size = head_size\n",
        "        self.T = T\n",
        "        self.C = C\n",
        "        self.window_size = window_size\n",
        "        # Linear transformations for queries, keys, and values\n",
        "        self.query = nn.Linear(T, head_size, bias=False, dtype=torch.float64)\n",
        "        self.key = nn.Linear(T, head_size, bias=False, dtype=torch.float64)\n",
        "        self.value = nn.Linear(T, head_size, bias=False, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        x = x.view(B, self.T, self.C)\n",
        "        x = x.permute(0, 2, 1)  # Permute to (B, C, T)\n",
        "\n",
        "        k = self.key(x)  # (B, C, head_size)\n",
        "        q = self.query(x)  # (B, C, head_size)\n",
        "        v = self.value(x)  # (B, C, head_size)\n",
        "\n",
        "        k = k.permute(0, 2, 1)  # (B, head_size, C)\n",
        "        q = q.permute(0, 2, 1)  # (B, head_size, C)\n",
        "\n",
        "        # Attention scores\n",
        "        attn = q @ k.transpose(-2, -1) * self.T**-0.5  # (B, C, head_size, head_size)\n",
        "\n",
        "        # Apply random masking to 40% of the attention weights\n",
        "        random_mask = torch.rand(attn.shape, device=attn.device, dtype=torch.float64) < 0.30\n",
        "        attn[random_mask] = float('-inf')\n",
        "\n",
        "        # Apply window-based masking\n",
        "        for i in range(self.head_size):\n",
        "            left = max(0, i - self.window_size)\n",
        "            right = min(self.head_size, i + self.window_size + 1)\n",
        "\n",
        "            # Mask out tokens outside the window\n",
        "            if left > 0:\n",
        "                attn[:, i, :left] = float('-inf')\n",
        "            if right < T:\n",
        "                attn[:, i, right:] = float('-inf')\n",
        "\n",
        "        # Ensure no row is fully masked\n",
        "        all_inf_mask = (attn == float('-inf')).all(dim=-1, keepdim=True)\n",
        "        attn = torch.where(all_inf_mask, torch.zeros_like(attn), attn)\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        out = v @ attn  # (B, C, head_size)\n",
        "        return out\n",
        "class MultiHeadAttentionEncoder(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, T, C):\n",
        "        super().__init__()\n",
        "        head_size = T // n_head\n",
        "        self.heads = nn.ModuleList([HeadEncoder(n_embd, head_size, T, C) for _ in range(n_head)])\n",
        "        self.proj = nn.Linear(head_size*n_head, T, dtype=torch.float64)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T = x.shape\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(out)\n",
        "        out = self.dropout(out)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        out = out.contiguous().view(B, -1)\n",
        "        return out\n",
        "\n",
        "class FeedForwardNetworkEncoder(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_embd, n_embd, dtype=torch.float64)\n",
        "        self.fc2 = nn.Linear(n_embd, n_embd, dtype=torch.float64)\n",
        "        self.dropout = nn.Dropout(0.30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ConvolutionalNetworkEncoder(nn.Module):\n",
        "    def __init__(self, T, C):\n",
        "        super().__init__()\n",
        "        self.T = T\n",
        "        self.C = C\n",
        "        # 1D convolution layers with kernel size covering time steps\n",
        "        self.conv1 = nn.Conv1d(in_channels=C, out_channels=C, kernel_size=5, padding=2, dtype=torch.float64)\n",
        "        self.conv2 = nn.Conv1d(in_channels=C, out_channels=C, kernel_size=5, padding=2, dtype=torch.float64)\n",
        "        self.conv3 = nn.Conv1d(in_channels=C, out_channels=C, kernel_size=5, padding=2, dtype=torch.float64)\n",
        "        self.dropout = nn.Dropout(0.30)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, TC = x.shape\n",
        "        assert TC == self.T * self.C, \"Input dimensions do not match T and C\"\n",
        "\n",
        "        # Reshape to [B, T, C]\n",
        "        x = x.view(B, self.T, self.C)\n",
        "\n",
        "        # Permute to [B, C, T] for convolution\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Apply the first 1D convolutional layer\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout(x)\n",
        "        # Apply the second 1D convolutional layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.conv3(x)\n",
        "\n",
        "        # Permute back to [B, T, C]\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # Reshape back to [B, T*C]\n",
        "        x = x.contiguous().view(B, -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class RegressionHead(nn.Module):\n",
        "    def __init__(self, n_input, n_hidden):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(n_input, n_hidden * 2, dtype=torch.float64)\n",
        "        self.fc2 = nn.Linear(n_hidden * 2, n_hidden, dtype=torch.float64)\n",
        "        self.fc3 = nn.Linear(n_hidden, n_hidden // 2, dtype=torch.float64)\n",
        "        self.fc4 = nn.Linear(n_hidden // 2, n_hidden // 4, dtype=torch.float64)\n",
        "        self.fc6 = nn.Linear(n_hidden // 4, 1, dtype=torch.float64)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc4(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc6(x)\n",
        "        return x\n",
        "\n",
        "class LSTMHead(nn.Module):\n",
        "    def __init__(self, n_embd, n_hidden, T, C):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_size=C, hidden_size=C, batch_first=True, dtype=torch.float64)\n",
        "        self.C = C\n",
        "        self.T = T\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, TC = x.shape\n",
        "        x = x.view(B, self.T, self.C)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x.contiguous().view(B, -1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, n_embd, n_head, T, C):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttentionEncoder(n_embd, n_head, T, C)\n",
        "        self.ffn = FeedForwardNetworkEncoder(n_embd)\n",
        "        self.lstm = LSTMHead(n_embd, n_hidden, T, C)\n",
        "        self.ln1 = nn.LayerNorm(n_embd, dtype=torch.float64)\n",
        "        self.ln2 = nn.LayerNorm(n_embd, dtype=torch.float64)\n",
        "        self.ln3 = nn.LayerNorm(n_embd, dtype=torch.float64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.self_attention(self.ln1(x))\n",
        "        x = x + self.ffn(self.ln2(x))\n",
        "        x = x + self.lstm(self.ln3(x))\n",
        "        return x\n",
        "\n",
        "class EncoderRegression(nn.Module):\n",
        "    def __init__(self, n_embd, n_layer, n_head, n_hidden, device, seed, max_seq_length, T, C):\n",
        "        super().__init__()\n",
        "        self.blocks = nn.Sequential(*[EncoderBlock(n_embd, n_head, T, C) for _ in range(n_layer)])\n",
        "        #self.lstm = LSTMHead(n_embd, n_hidden, T, C)\n",
        "        self.regression_head = RegressionHead(n_embd, n_hidden)\n",
        "        self.ln_f = nn.LayerNorm(n_embd, dtype=torch.float64)\n",
        "        self.positional_embeddings = nn.Parameter(torch.zeros(1, C, T, dtype=torch.float64))\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        torch.manual_seed(seed)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, xbatch, ybatch=None):\n",
        "        B, T, C = xbatch.shape\n",
        "\n",
        "        # Reshape input to (B, C, T)\n",
        "        xbatch = xbatch.permute(0, 2, 1)  # (B, T, C) -> (B, C, T)\n",
        "\n",
        "        # Adding positional embeddings\n",
        "        pos_emb = self.positional_embeddings[:, :, :T]  # (1, C, T)\n",
        "        xbatch = xbatch + pos_emb\n",
        "\n",
        "        # Reshape input to (B, T, C)\n",
        "        xbatch = xbatch.permute(0, 2, 1)  # (B, C, T)\n",
        "        xbatch = xbatch.reshape(xbatch.size(0), -1)\n",
        "\n",
        "        # Process through encoder blocks\n",
        "        input = self.blocks(xbatch)\n",
        "\n",
        "        # Layer normalization\n",
        "        input = self.ln_f(input)\n",
        "\n",
        "        out = self.regression_head(input)\n",
        "\n",
        "        # Compute loss if ybatch is provided\n",
        "        loss = None\n",
        "        if ybatch is not None:\n",
        "            loss = F.l1_loss(out, ybatch.unsqueeze(1).double())\n",
        "\n",
        "        return out, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwVOEqCxQ8Z-"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        "    def __init__(self, patience=7, verbose=False, delta=0, fold=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "        self.val_loss_min = np.Inf\n",
        "        self.delta = delta\n",
        "        self.fold = fold\n",
        "\n",
        "    def __call__(self, val_loss, model):\n",
        "        score = -val_loss\n",
        "\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.verbose:\n",
        "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.save_checkpoint(val_loss, model)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, val_loss, model):\n",
        "        if self.verbose:\n",
        "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
        "        torch.save(model.state_dict(), f'checkpoint_fold_{self.fold}.pth')\n",
        "        self.val_loss_min = val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Cross-validation parameters\n",
        "n_splits_k_fold = 5\n",
        "epochs = 2000#1000\n",
        "patience = 10\n",
        "seed = 42\n",
        "learning_rate = 0.00005 #0.00005\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "kf = KFold(n_splits=n_splits_k_fold, shuffle=True, random_state=seed)\n",
        "train_losses_per_fold = []\n",
        "val_losses_per_fold = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "    print(f'Fold {fold+1}')\n",
        "    X_train, X_val = X[train_idx], X[val_idx]\n",
        "    y_train, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "    train_dataset = TimeSeriesDataset(X_train, y_train)\n",
        "    val_dataset = TimeSeriesDataset(X_val, y_val)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "    model = EncoderRegression(n_embd=n_embd, n_layer=n_layer, n_head=n_head, n_hidden=n_hidden, device=device, seed=seed, max_seq_length=max_seq_length, T=T, C=C).to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)#lr = 0.00001\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    early_stopping = EarlyStopping(patience=patience, verbose=True, fold=fold+1)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):  # Number of epochs\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        for xbatch, ybatch in train_loader:\n",
        "            xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            out, loss = model(xbatch, ybatch)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            running_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = running_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for xbatch, ybatch in val_loader:\n",
        "                xbatch, ybatch = xbatch.to(device), ybatch.to(device)\n",
        "                out, loss = model(xbatch, ybatch)\n",
        "                running_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = running_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        if epoch % 5 == 0:\n",
        "            print(f'Epoch [{epoch+1}/{epochs}], Fold {fold+1}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Check early stopping\n",
        "        early_stopping(avg_val_loss, model)\n",
        "\n",
        "        if early_stopping.early_stop:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "    train_losses_per_fold.append(train_losses)\n",
        "    val_losses_per_fold.append(val_losses)\n",
        "\n",
        "    # Load the last checkpoint with the best model for this fold\n",
        "    model.load_state_dict(torch.load(f'checkpoint_fold_{fold+1}.pth'))\n",
        "\n",
        "# Plotting learning curves for each fold\n",
        "fig, axes = plt.subplots(n_splits_k_fold, 1, figsize=(10, 25), sharex=True)\n",
        "\n",
        "for fold in range(n_splits_k_fold):\n",
        "    axes[fold].plot(train_losses_per_fold[fold], label='Training Loss')\n",
        "    axes[fold].plot(val_losses_per_fold[fold], label='Validation Loss')\n",
        "    axes[fold].set_title(f'Fold {fold+1}')\n",
        "    axes[fold].set_xlabel('Epoch')\n",
        "    axes[fold].set_ylabel('Loss')\n",
        "    axes[fold].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YLQDJVShFYXY"
      },
      "outputs": [],
      "source": [
        "# Flatten the lists\n",
        "train_losses = [item for sublist in train_losses_per_fold for item in sublist]\n",
        "val_losses = [item for sublist in val_losses_per_fold for item in sublist]\n",
        "\n",
        "# Determine fold boundaries\n",
        "fold_sizes = [len(fold) for fold in train_losses_per_fold]\n",
        "fold_boundaries = [sum(fold_sizes[:i+1]) for i in range(len(fold_sizes))]\n",
        "\n",
        "# Plotting learning curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "\n",
        "# Adding red dashed lines to separate each fold\n",
        "for boundary in fold_boundaries:\n",
        "    plt.axvline(x=boundary, color='red', linestyle='--', linewidth=1)\n",
        "\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdtBDpVEupZI"
      },
      "source": [
        "# Import Test Data and Predicting it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PID-pqkrGFVK"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('checkpoint_fold_5.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPwLo-VZRiu2"
      },
      "outputs": [],
      "source": [
        "B, T, C = X_test.shape\n",
        "\n",
        "print(T)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWcqAcDxtTSH"
      },
      "outputs": [],
      "source": [
        "test_dataset = TimeSeriesDataset(X_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "# Placeholder for storing predictions from each fold\n",
        "predictions_per_fold = []\n",
        "\n",
        "for fold in range(n_splits_k_fold):\n",
        "    # Initialize the model\n",
        "    model = EncoderRegression(n_embd=n_embd, n_layer=n_layer, n_head=n_head, n_hidden=n_hidden, device=device, seed=seed, max_seq_length=max_seq_length, T=T, C=C).to(device)\n",
        "\n",
        "    # Load the best model for the current fold\n",
        "    model.load_state_dict(torch.load(f'checkpoint_fold_{fold+1}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # Collect predictions\n",
        "    fold_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for xbatch in test_loader:\n",
        "            xbatch = xbatch.to(device)\n",
        "            out, _ = model(xbatch)  # Assuming model(xbatch) returns (output, loss)\n",
        "            fold_predictions.append(out.cpu().numpy())\n",
        "\n",
        "    # Concatenate predictions for the current fold\n",
        "    fold_predictions = np.concatenate(fold_predictions, axis=0)\n",
        "    predictions_per_fold.append(fold_predictions)\n",
        "\n",
        "# Convert the list of predictions to a NumPy array for easier manipulation\n",
        "predictions_per_fold = np.array(predictions_per_fold)\n",
        "\n",
        "# Average the predictions from all folds\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klDi_2xXv34v"
      },
      "outputs": [],
      "source": [
        "#df_test = pd.read_csv('test.csv')\n",
        "df_test = pd.read_csv('/content/asset_id_corresponding_to_X_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb8U8smtWYIg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDx0wt_XvJBb"
      },
      "outputs": [],
      "source": [
        "predictions = [i[0] for i in predictions_per_fold[3]]\n",
        "\n",
        "\n",
        "submission_file=pd.DataFrame()\n",
        "assets=pd.Series(df_test.asset_id.unique()).rename('asset_id')\n",
        "predictions=pd.Series(predictions).rename('pediction')\n",
        "submission_file=pd.concat([assets,predictions],axis=1)\n",
        "submission_file.to_csv('Submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YtvIbeIQPTvc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RHpYj_ogwVG-"
      },
      "outputs": [],
      "source": [
        "Ensemble_predictions = np.mean(predictions_per_fold[[0,1,3]], axis=0)\n",
        "\n",
        "predictions = [i[0] for i in Ensemble_predictions]\n",
        "\n",
        "submission_file=pd.DataFrame()\n",
        "assets=pd.Series(df_test.asset_id.unique()).rename('asset_id')\n",
        "predictions=pd.Series(predictions).rename('pediction')\n",
        "submission_file=pd.concat([assets,predictions],axis=1)\n",
        "submission_file.to_csv('Ensemble_predictions.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fsHOnX-A3HXE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}